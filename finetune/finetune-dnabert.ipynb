{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccf62c9-64ac-46e8-aa14-c7b6d5ee121c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import copy\n",
    "import json\n",
    "import logging\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, Dict, Sequence, Tuple, List\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "import sklearn\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    ")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelArguments:\n",
    "    model_name_or_path: Optional[str] = field(default=\"facebook/opt-125m\")\n",
    "    use_lora: bool = field(default=False, metadata={\"help\": \"whether to use LoRA\"})\n",
    "    lora_r: int = field(default=8, metadata={\"help\": \"hidden dimension for LoRA\"})\n",
    "    lora_alpha: int = field(default=32, metadata={\"help\": \"alpha for LoRA\"})\n",
    "    lora_dropout: float = field(default=0.05, metadata={\"help\": \"dropout rate for LoRA\"})\n",
    "    lora_target_modules: str = field(default=\"query,value\", metadata={\"help\": \"where to perform LoRA\"})\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataArguments:\n",
    "    data_path: str = field(default=None, metadata={\"help\": \"Path to the training data.\"})\n",
    "    kmer: int = field(default=-1, metadata={\"help\": \"k-mer for input sequence. -1 means not using k-mer.\"})\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainingArguments(transformers.TrainingArguments):\n",
    "    cache_dir: Optional[str] = field(default=None)\n",
    "    run_name: str = field(default=\"run\")\n",
    "    optim: str = field(default=\"adamw_torch\")\n",
    "    model_max_length: int = field(default=512, metadata={\"help\": \"Maximum sequence length.\"})\n",
    "    gradient_accumulation_steps: int = field(default=1)\n",
    "    per_device_train_batch_size: int = field(default=1)\n",
    "    per_device_eval_batch_size: int = field(default=1)\n",
    "    num_train_epochs: int = field(default=1)\n",
    "    fp16: bool = field(default=False)\n",
    "    logging_steps: int = field(default=100)\n",
    "    save_steps: int = field(default=100)\n",
    "    eval_steps: int = field(default=100)\n",
    "    evaluation_strategy: str = field(default=\"steps\"),\n",
    "    warmup_steps: int = field(default=50)\n",
    "    weight_decay: float = field(default=0.01)\n",
    "    learning_rate: float = field(default=1e-4)\n",
    "    save_total_limit: int = field(default=3)\n",
    "    load_best_model_at_end: bool = field(default=True)\n",
    "    output_dir: str = field(default=\"output\")\n",
    "    find_unused_parameters: bool = field(default=False)\n",
    "    checkpointing: bool = field(default=False)\n",
    "    dataloader_pin_memory: bool = field(default=False)\n",
    "    eval_and_save_results: bool = field(default=True)\n",
    "    save_model: bool = field(default=False)\n",
    "    seed: int = field(default=42)\n",
    "    \n",
    "\n",
    "def safe_save_model_for_hf_trainer(trainer: transformers.Trainer, output_dir: str):\n",
    "    \"\"\"Collects the state dict and dump to disk.\"\"\"\n",
    "    state_dict = trainer.model.state_dict()\n",
    "    if trainer.args.should_save:\n",
    "        cpu_state_dict = {key: value.cpu() for key, value in state_dict.items()}\n",
    "        del state_dict\n",
    "        trainer._save(output_dir, state_dict=cpu_state_dict)  # noqa\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Get the reversed complement of the original DNA sequence.\n",
    "\"\"\"\n",
    "def get_alter_of_dna_sequence(sequence: str):\n",
    "    MAP = {\"A\": \"T\", \"T\": \"A\", \"C\": \"G\", \"G\": \"C\"}\n",
    "    # return \"\".join([MAP[c] for c in reversed(sequence)])\n",
    "    return \"\".join([MAP[c] for c in sequence])\n",
    "\n",
    "\"\"\"\n",
    "Transform a dna sequence to k-mer string\n",
    "\"\"\"\n",
    "def generate_kmer_str(sequence: str, k: int) -> str:\n",
    "    \"\"\"Generate k-mer string from DNA sequence.\"\"\"\n",
    "    return \" \".join([sequence[i:i+k] for i in range(len(sequence) - k + 1)])\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Load or generate k-mer string for each DNA sequence. The generated k-mer string will be saved to the same directory as the original data with the same name but with a suffix of \"_{k}mer\".\n",
    "\"\"\"\n",
    "def load_or_generate_kmer(data_path: str, texts: List[str], k: int) -> List[str]:\n",
    "    \"\"\"Load or generate k-mer string for each DNA sequence.\"\"\"\n",
    "    kmer_path = data_path.replace(\".csv\", f\"_{k}mer.json\")\n",
    "    if os.path.exists(kmer_path):\n",
    "        logging.warning(f\"Loading k-mer from {kmer_path}...\")\n",
    "        with open(kmer_path, \"r\") as f:\n",
    "            kmer = json.load(f)\n",
    "    else:        \n",
    "        logging.warning(f\"Generating k-mer...\")\n",
    "        kmer = [generate_kmer_str(text, k) for text in texts]\n",
    "        with open(kmer_path, \"w\") as f:\n",
    "            logging.warning(f\"Saving k-mer to {kmer_path}...\")\n",
    "            json.dump(kmer, f)\n",
    "        \n",
    "    return kmer\n",
    "\n",
    "class SupervisedDataset(Dataset):\n",
    "    \"\"\"Dataset for supervised fine-tuning.\"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 data_path: str, \n",
    "                 tokenizer: transformers.PreTrainedTokenizer, \n",
    "                 kmer: int = -1):\n",
    "\n",
    "        super(SupervisedDataset, self).__init__()\n",
    "\n",
    "        # load data from the disk\n",
    "        with open(data_path, \"r\") as f:\n",
    "            data = list(csv.reader(f))[1:]\n",
    "        if len(data[0]) == 2:\n",
    "            # data is in the format of [text, label]\n",
    "            logging.warning(\"Perform single sequence classification...\")\n",
    "            texts = [d[0] for d in data]\n",
    "            labels = [int(d[1]) for d in data]\n",
    "        elif len(data[0]) == 3:\n",
    "            # data is in the format of [text1, text2, label]\n",
    "            logging.warning(\"Perform sequence-pair classification...\")\n",
    "            texts = [[d[0], d[1]] for d in data]\n",
    "            labels = [int(d[2]) for d in data]\n",
    "        else:\n",
    "            raise ValueError(\"Data format not supported.\")\n",
    "        \n",
    "        if kmer != -1:\n",
    "            # only write file on the first process\n",
    "            if torch.distributed.get_rank() not in [0, -1]:\n",
    "                torch.distributed.barrier()\n",
    "\n",
    "            logging.warning(f\"Using {kmer}-mer as input...\")\n",
    "            texts = load_or_generate_kmer(data_path, texts, kmer)\n",
    "\n",
    "            if torch.distributed.get_rank() == 0:\n",
    "                torch.distributed.barrier()\n",
    "\n",
    "        output = tokenizer(\n",
    "            texts,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"longest\",\n",
    "            max_length=tokenizer.model_max_length,\n",
    "            truncation=True,\n",
    "        )\n",
    "\n",
    "        self.input_ids = output[\"input_ids\"]\n",
    "        self.attention_mask = output[\"attention_mask\"]\n",
    "        self.labels = labels\n",
    "        self.num_labels = len(set(labels))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
    "        return dict(input_ids=self.input_ids[i], labels=self.labels[i])\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorForSupervisedDataset(object):\n",
    "    \"\"\"Collate examples for supervised fine-tuning.\"\"\"\n",
    "\n",
    "    tokenizer: transformers.PreTrainedTokenizer\n",
    "\n",
    "    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n",
    "        input_ids, labels = tuple([instance[key] for instance in instances] for key in (\"input_ids\", \"labels\"))\n",
    "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "            input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
    "        )\n",
    "        labels = torch.Tensor(labels).long()\n",
    "        return dict(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels,\n",
    "            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),\n",
    "        )\n",
    "\n",
    "\"\"\"\n",
    "Manually calculate the accuracy, f1, matthews_correlation, precision, recall with sklearn.\n",
    "\"\"\"\n",
    "def calculate_metric_with_sklearn(logits: np.ndarray, labels: np.ndarray):\n",
    "    if logits.ndim == 3:\n",
    "        # Reshape logits to 2D if needed\n",
    "        logits = logits.reshape(-1, logits.shape[-1])\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    valid_mask = labels != -100  # Exclude padding tokens (assuming -100 is the padding token ID)\n",
    "    valid_predictions = predictions[valid_mask]\n",
    "    valid_labels = labels[valid_mask]\n",
    "    return {\n",
    "        \"accuracy\": sklearn.metrics.accuracy_score(valid_labels, valid_predictions),\n",
    "        \"f1\": sklearn.metrics.f1_score(\n",
    "            valid_labels, valid_predictions, average=\"macro\", zero_division=0\n",
    "        ),\n",
    "        \"matthews_correlation\": sklearn.metrics.matthews_corrcoef(\n",
    "            valid_labels, valid_predictions\n",
    "        ),\n",
    "        \"precision\": sklearn.metrics.precision_score(\n",
    "            valid_labels, valid_predictions, average=\"macro\", zero_division=0\n",
    "        ),\n",
    "        \"recall\": sklearn.metrics.recall_score(\n",
    "            valid_labels, valid_predictions, average=\"macro\", zero_division=0\n",
    "        ),\n",
    "    }\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Compute metrics used for huggingface trainer.\n",
    "\"\"\" \n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    if isinstance(logits, tuple):  # Unpack logits if it's a tuple\n",
    "        logits = logits[0]\n",
    "    return calculate_metric_with_sklearn(logits, labels)\n",
    "\n",
    "\n",
    "\n",
    "def train():\n",
    "    parser = transformers.HfArgumentParser((ModelArguments, DataArguments, TrainingArguments))\n",
    "    model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n",
    "\n",
    "    # load tokenizer\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "        model_args.model_name_or_path,\n",
    "        cache_dir=training_args.cache_dir,\n",
    "        model_max_length=training_args.model_max_length,\n",
    "        padding_side=\"right\",\n",
    "        use_fast=True,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "\n",
    "    if \"InstaDeepAI\" in model_args.model_name_or_path:\n",
    "        tokenizer.eos_token = tokenizer.pad_token\n",
    "\n",
    "    # define datasets and data collator\n",
    "    train_dataset = SupervisedDataset(tokenizer=tokenizer, \n",
    "                                      data_path=os.path.join(data_args.data_path, \"train.csv\"), \n",
    "                                      kmer=data_args.kmer)\n",
    "    val_dataset = SupervisedDataset(tokenizer=tokenizer, \n",
    "                                     data_path=os.path.join(data_args.data_path, \"dev.csv\"), \n",
    "                                     kmer=data_args.kmer)\n",
    "    test_dataset = SupervisedDataset(tokenizer=tokenizer, \n",
    "                                     data_path=os.path.join(data_args.data_path, \"test.csv\"), \n",
    "                                     kmer=data_args.kmer)\n",
    "    data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "    # load model\n",
    "    model = transformers.AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_args.model_name_or_path,\n",
    "        cache_dir=training_args.cache_dir,\n",
    "        num_labels=train_dataset.num_labels,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "\n",
    "    # configure LoRA\n",
    "    if model_args.use_lora:\n",
    "        lora_config = LoraConfig(\n",
    "            r=model_args.lora_r,\n",
    "            lora_alpha=model_args.lora_alpha,\n",
    "            target_modules=list(model_args.lora_target_modules.split(\",\")),\n",
    "            lora_dropout=model_args.lora_dropout,\n",
    "            bias=\"none\",\n",
    "            task_type=\"SEQ_CLS\",\n",
    "            inference_mode=False,\n",
    "        )\n",
    "        model = get_peft_model(model, lora_config)\n",
    "        model.print_trainable_parameters()\n",
    "\n",
    "    # define trainer\n",
    "    trainer = transformers.Trainer(model=model,\n",
    "                                   tokenizer=tokenizer,\n",
    "                                   args=training_args,\n",
    "                                   compute_metrics=compute_metrics,\n",
    "                                   train_dataset=train_dataset,\n",
    "                                   eval_dataset=val_dataset,\n",
    "                                   data_collator=data_collator)\n",
    "    trainer.train()\n",
    "\n",
    "    if training_args.save_model:\n",
    "        trainer.save_state()\n",
    "        safe_save_model_for_hf_trainer(trainer=trainer, output_dir=training_args.output_dir)\n",
    "\n",
    "    # get the evaluation results from trainer\n",
    "    if training_args.eval_and_save_results:\n",
    "        results_path = os.path.join(training_args.output_dir, \"results\", training_args.run_name)\n",
    "        results = trainer.evaluate(eval_dataset=test_dataset)\n",
    "        os.makedirs(results_path, exist_ok=True)\n",
    "        with open(os.path.join(results_path, \"eval_results.json\"), \"w\") as f:\n",
    "            json.dump(results, f)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b96baac-bbb7-496b-b87f-9dfa8e512bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "\n",
    "cd finetune\n",
    "\n",
    "export DATA_PATH=\"./sample_data\"  # e.g., ./sample_data\n",
    "export MAX_LENGTH=100 # Please set the number as 0.25 * your sequence length. \n",
    "\t\t\t\t\t\t\t\t\t\t\t# e.g., set it as 250 if your DNA sequences have 1000 nucleotide bases\n",
    "\t\t\t\t\t\t\t\t\t\t\t# This is because the tokenized will reduce the sequence length by about 5 times\n",
    "export LR=3e-5\n",
    "\n",
    "# Training use DataParallel\n",
    "python train.py \\\n",
    "    --model_name_or_path zhihan1996/DNABERT-2-117M \\\n",
    "    --data_path  ${DATA_PATH} \\\n",
    "    --kmer -1 \\\n",
    "    --run_name DNABERT2_${DATA_PATH} \\\n",
    "    --model_max_length ${MAX_LENGTH} \\\n",
    "    --per_device_train_batch_size 8 \\\n",
    "    --per_device_eval_batch_size 16 \\\n",
    "    --gradient_accumulation_steps 1 \\\n",
    "    --learning_rate ${LR} \\\n",
    "    --num_train_epochs 5 \\\n",
    "    --fp16 \\\n",
    "    --save_steps 200 \\\n",
    "    --output_dir output/dnabert2 \\\n",
    "    --evaluation_strategy steps \\\n",
    "    --eval_steps 200 \\\n",
    "    --warmup_steps 50 \\\n",
    "    --logging_steps 100 \\\n",
    "    --overwrite_output_dir True \\\n",
    "    --log_level info \\\n",
    "    --find_unused_parameters False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aac664c-5296-4914-88de-e7f970a0e7f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0798572a-dddf-4817-9fe4-11a95fce1128",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4be933-755e-4eda-ae13-ab0531f09ec2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35dba8f-45ee-423a-87f5-ad8d30394296",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3ab1c3-2686-4b98-9805-b9fc8ce9784d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b238f2-e32d-4a36-9e40-68378a2bffeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44d25f6-4b09-4d97-a2c3-22d62016f962",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
