{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1748588-c875-4ca7-80e2-7dac121b5d78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/users/anup/miniconda3/envs/finetune-dnabert2/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "import tqdm\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from multiprocessing import Pool\n",
    "#from functools import partial\n",
    "from rouge_score import rouge_scorer\n",
    "#from gpt3_api import make_requests as make_gpt3_requests\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, GenerationConfig, LlamaForCausalLM, LlamaTokenizer\n",
    "\n",
    "import torch\n",
    "from peft import PeftModel\n",
    "\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e19e5cfd-ae87-4792-a9c0-21048ff4fbd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'if torch.cuda.is_available():\\n    device = \"cuda\"\\nelse:\\n    device = \"CPU\"\\n\\nload_8bit = True\\nbase_model = \"decapoda-research/llama-7b-hf\" #\"decapoda-research/llama-7b-hf\"\\nlora_weights = \"tloen/alpaca-lora-7b\" #\"tloen/alpaca-lora-7b\"\\n\\nassert (\\n    base_model\\n), \"Please specify a --base_model, e.g. --base_model=\\'huggyllama/llama-7b\\'\"\\n\\n\\ntokenizer = LlamaTokenizer.from_pretrained(base_model)\\n\\nif device == \"cuda\":\\n    model = LlamaForCausalLM.from_pretrained(\\n        base_model,\\n        load_in_8bit=load_8bit,\\n        torch_dtype=torch.float16,\\n        device_map=\"auto\",\\n    )\\n    model = PeftModel.from_pretrained(\\n        model,\\n        lora_weights,\\n        torch_dtype=torch.float16,\\n    )\\nelif device == \"mps\":\\n    model = LlamaForCausalLM.from_pretrained(\\n        base_model,\\n        device_map={\"\": device},\\n        torch_dtype=torch.float16,\\n    )\\n    model = PeftModel.from_pretrained(\\n        model,\\n        lora_weights,\\n        device_map={\"\": device},\\n        torch_dtype=torch.float16,\\n    )\\nelse:\\n    model = LlamaForCausalLM.from_pretrained(\\n        base_model, device_map={\"\": device}, low_cpu_mem_usage=True\\n    )\\n    model = PeftModel.from_pretrained(\\n        model,\\n        lora_weights,\\n        device_map={\"\": device},\\n    )\\n\\n# unwind broken decapoda-research config\\nmodel.config.pad_token_id = tokenizer.pad_token_id = 0  # unk\\nmodel.config.bos_token_id = 1\\nmodel.config.eos_token_id = 2\\n\\nif not load_8bit:\\n    model.half()  # seems to fix bugs for some users.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from utils.callbacks import Iteratorize, Stream\n",
    "#from utils.prompter import Prompter\n",
    "\n",
    "'''if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"CPU\"\n",
    "\n",
    "load_8bit = True\n",
    "base_model = \"decapoda-research/llama-7b-hf\" #\"decapoda-research/llama-7b-hf\"\n",
    "lora_weights = \"tloen/alpaca-lora-7b\" #\"tloen/alpaca-lora-7b\"\n",
    "\n",
    "assert (\n",
    "    base_model\n",
    "), \"Please specify a --base_model, e.g. --base_model='huggyllama/llama-7b'\"\n",
    "\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(base_model)\n",
    "\n",
    "if device == \"cuda\":\n",
    "    model = LlamaForCausalLM.from_pretrained(\n",
    "        base_model,\n",
    "        load_in_8bit=load_8bit,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "    model = PeftModel.from_pretrained(\n",
    "        model,\n",
    "        lora_weights,\n",
    "        torch_dtype=torch.float16,\n",
    "    )\n",
    "elif device == \"mps\":\n",
    "    model = LlamaForCausalLM.from_pretrained(\n",
    "        base_model,\n",
    "        device_map={\"\": device},\n",
    "        torch_dtype=torch.float16,\n",
    "    )\n",
    "    model = PeftModel.from_pretrained(\n",
    "        model,\n",
    "        lora_weights,\n",
    "        device_map={\"\": device},\n",
    "        torch_dtype=torch.float16,\n",
    "    )\n",
    "else:\n",
    "    model = LlamaForCausalLM.from_pretrained(\n",
    "        base_model, device_map={\"\": device}, low_cpu_mem_usage=True\n",
    "    )\n",
    "    model = PeftModel.from_pretrained(\n",
    "        model,\n",
    "        lora_weights,\n",
    "        device_map={\"\": device},\n",
    "    )\n",
    "\n",
    "# unwind broken decapoda-research config\n",
    "model.config.pad_token_id = tokenizer.pad_token_id = 0  # unk\n",
    "model.config.bos_token_id = 1\n",
    "model.config.eos_token_id = 2\n",
    "\n",
    "if not load_8bit:\n",
    "    model.half()  # seems to fix bugs for some users.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa33fab4-7974-407e-b29d-edca00295a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"stanford-crfm/BioMedLM\")\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"stanford-crfm/BioMedLM\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eecf3453-d74c-4d20-8519-4bbd8794bb71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39499417-d2b0-4b60-8970-408719d41df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A dedicated helper to manage templates and prompt building.\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import os.path as osp\n",
    "from typing import Union\n",
    "\n",
    "\n",
    "class Prompter(object):\n",
    "    __slots__ = (\"template\", \"_verbose\")\n",
    "\n",
    "    def __init__(self, template_name: str = \"\", verbose: bool = False):\n",
    "        self._verbose = verbose\n",
    "        if not template_name:\n",
    "            # Enforce the default here, so the constructor can be called with '' and will not break.\n",
    "            template_name = \"alpaca\"\n",
    "        file_name = osp.join(\"templates\", f\"{template_name}.json\")\n",
    "        if not osp.exists(file_name):\n",
    "            raise ValueError(f\"Can't read {file_name}\")\n",
    "        with open(file_name) as fp:\n",
    "            self.template = json.load(fp)\n",
    "        if self._verbose:\n",
    "            print(\n",
    "                f\"Using prompt template {template_name}: {self.template['description']}\"\n",
    "            )\n",
    "\n",
    "    def generate_prompt(\n",
    "        self,\n",
    "        instruction: str,\n",
    "        input: Union[None, str] = None,\n",
    "        label: Union[None, str] = None,\n",
    "    ) -> str:\n",
    "        # returns the full prompt from instruction and optional input\n",
    "        # if a label (=response, =output) is provided, it's also appended.\n",
    "        if input:\n",
    "            res = self.template[\"prompt_input\"].format(\n",
    "                instruction=instruction, input=input\n",
    "            )\n",
    "        else:\n",
    "            res = self.template[\"prompt_no_input\"].format(\n",
    "                instruction=instruction\n",
    "            )\n",
    "        if label:\n",
    "            res = f\"{res}{label}\"\n",
    "        if self._verbose:\n",
    "            print(res)\n",
    "        return res\n",
    "\n",
    "    def get_response(self, output: str) -> str:\n",
    "        return output #output.split(self.template[\"response_split\"])[1].strip()\n",
    "\n",
    "prompt_template = \"alpaca_1\"\n",
    "prompter = Prompter(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1fa68d71-7793-41fe-b87c-875a210cbd13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_dir': 'data/gpt3_generations/', 'seed_tasks_path': 'data/galaxy-qa.jsonl', 'num_instructions_to_generate': 1, 'num_prompt_instructions': 1, 'request_batch_size': 10, 'use_clf_seed_tasks_only': False}\n"
     ]
    }
   ],
   "source": [
    "args = {\"batch_dir\": \"data/gpt3_generations/\",\n",
    "       \"seed_tasks_path\": \"data/galaxy-qa.jsonl\", #\"data/galaxy-qa.jsonl\",\n",
    "       \"num_instructions_to_generate\": 1,\n",
    "        \"num_prompt_instructions\": 1,\n",
    "        \"request_batch_size\": 10,\n",
    "        \"use_clf_seed_tasks_only\": False\n",
    "       }\n",
    "print(args)\n",
    "\n",
    "def encode_prompt(prompt_instructions, classification=False):\n",
    "    \"\"\"Encode multiple prompt instructions into a single string.\"\"\"\n",
    "    if classification:\n",
    "        prompt = \"Come up with a series of classification tasks. Try to specify the possible output labels when possible.\\n\"\n",
    "    else:\n",
    "        #prompt = \"Come up with a series of tasks:\\n\"\n",
    "        prompt = \"Come up with a series of tasks: \\n\"\n",
    "        #\"Come up with a new question using scientific keywords mentioned in existing tasks: \\n\"\n",
    "        #\"Come up with a series of new tasks using scientific keywords mentioned in existing tasks: \\n\" \n",
    "        #\"Come up with a series of new tasks: \\n\"\n",
    "    for idx, instruction in enumerate(prompt_instructions):\n",
    "        instruction = re.sub(r\"\\s+\", \" \", instruction).strip().rstrip(\":\")\n",
    "        prompt += f\"{idx+1}. {instruction}\\n\"\n",
    "    #prompt += f\"{len(prompt_instructions) + 1}.\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def sample_machine_instructions(machine_instructions, similarities, n):\n",
    "    \"\"\"Sample n machine instructions from a list of machine instructions.\"\"\"\n",
    "    return random.sample(machine_instructions, min(n, len(machine_instructions)))\n",
    "\n",
    "\n",
    "def find_word_in_string(w, s):\n",
    "    return re.compile(r'\\b({0})\\b'.format(w), flags=re.IGNORECASE).search(s)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1f8a646a-8cba-4287-9dd3-08cd0d81f65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(\n",
    "    instruction,\n",
    "    input=None,\n",
    "    temperature=0.1, #0.7, #0.1,\n",
    "    top_p=0.75, #0.5, #0.75,\n",
    "    top_k=40,\n",
    "    num_beams=4,\n",
    "    max_new_tokens=128, #128,\n",
    "    #max_tokens=1024,\n",
    "    stream_output=False\n",
    "):\n",
    "    prompt = instruction #prompter.generate_prompt(instruction, input)\n",
    "    print(prompt)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "    \n",
    "    generation_config = GenerationConfig(\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        top_k=top_k,\n",
    "        num_beams=num_beams,\n",
    "    )\n",
    "\n",
    "    generate_params = {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"generation_config\": generation_config,\n",
    "        \"return_dict_in_generate\": True,\n",
    "        \"output_scores\": True,\n",
    "        \"max_new_tokens\": max_new_tokens,\n",
    "    }\n",
    "\n",
    "    # Without streaming\n",
    "    with torch.no_grad():\n",
    "        generation_output = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            generation_config=generation_config,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "        )\n",
    "    s = generation_output.sequences[0]\n",
    "    output = tokenizer.decode(s)\n",
    "    return prompter.get_response(output)\n",
    "    #return dir(generation_output)\n",
    "    #print(generation_output)\n",
    "    #s = generation_output.sequences[0]\n",
    "    #output = tokenizer.decode(generation_output)\n",
    "    #print(s)\n",
    "    #yield output #prompter.get_response(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f6d16d0c-02f4-4bb5-82c6-eabd925e8ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_pubmedgpt(instruction):\n",
    "    input_ids = tokenizer.encode(\n",
    "        instruction, return_tensors=\"pt\").to(device)\n",
    "    sample_output = model.generate(input_ids, do_sample=True, max_length=128, temperature=0.7, top_k=50)\n",
    "\n",
    "    print(\"Output:\\n\" + 100 * \"-\")\n",
    "    print(sample_output.shape)\n",
    "    print(\"Output:\\n\" + 100 * \"-\")\n",
    "    #print(tokenizer.decode(sample_output[0], skip_special_tokens=True))\n",
    "    return tokenizer.decode(sample_output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "513f5d5c-ac9c-4b84-bd47-41ac0663a63d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 8 human-written seed instructions\n"
     ]
    }
   ],
   "source": [
    "seed_tasks = [json.loads(l) for l in open(args[\"seed_tasks_path\"], \"r\")]\n",
    "if args[\"use_clf_seed_tasks_only\"]:\n",
    "    seed_tasks = [t for t in seed_tasks if t[\"is_classification\"]]\n",
    "seed_instructions = [t[\"instruction\"] for t in seed_tasks]\n",
    "print(f\"Loaded {len(seed_instructions)} human-written seed instructions\")\n",
    "\n",
    "os.makedirs(args[\"batch_dir\"], exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336d09b3-1934-4669-90f2-d3f46370a412",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96645beb-f537-4ed4-8c7f-5c9e62a8e283",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f1a5756c-8d26-41e9-820b-be00158598ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                             | 0/1 [07:28<?, ?it/s]\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "torch.Size([1, 128])\n",
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Come up with a series of tasks: \n",
      "1. Why do we need to barcode a read transcript too? Isn’t mapping it against the reference genome enough?\n",
      "=================================================================================\n",
      "---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "torch.Size([1, 128])\n",
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Come up with a series of tasks: \n",
      "1. Does the distribution of target identifications differ from the decoy distribution?\n",
      "\n",
      "2. How does the number of targets identified depend on the number of decoys?\n",
      "\n",
      "3. Are there any systematic biases?\n",
      "\n",
      "4. How does the distribution of uniqueness differ from the decoy distribution?\n",
      "\n",
      "5. How does the number of uniqueness matches depend on the number of decoys?\n",
      "\n",
      "We used the same set of decoys and target sequences used in the original paper, which consisted of a subset of the PDB\n",
      "---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "torch.Size([1, 128])\n",
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Come up with a series of tasks: \n",
      "1. Does the distribution of target identifications differ from the decoy distribution?\n",
      "\n",
      "2. Are there any regions in which the target identification rate is significantly different from the decoy identification rate?\n",
      "\n",
      "3. How do we use the data in these regions to improve the performance of the search?\n",
      "\n",
      "To answer these questions, we need to know the distribution of the target and decoy identifications in the data (these are the expected values of the data) and the standard deviation of the data (the expected value of the error).\n",
      "\n",
      "There are two\n",
      "---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "torch.Size([1, 128])\n",
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Come up with a series of tasks: \n",
      "1. Can the same UMI map to different mRNA molecules of the same gene?\n",
      "\n",
      "2. Can a UMI barcode map to a single mRNA molecule?\n",
      "\n",
      "3. Can the same UMI match different mRNAs of the same gene?\n",
      "\n",
      "4. Do PCR duplicates occur at rates that are compatible with the intended sequencing depth?\n",
      "\n",
      "5. Does the number of unique UMIs in a population of sequences map to a Poisson distribution?\n",
      "\n",
      "6. Is the error rate for UMIs as low as possible?\n",
      "\n",
      "7.\n",
      "---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "torch.Size([1, 128])\n",
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Come up with a series of tasks: \n",
      "1. What do UMIs do?\n",
      "====================\n",
      "\n",
      "UMIs are primer sites that are incorporated into the DNA during PCR amplification and serve as a means of identifying individual template molecules ([Figure 1](#F1){ref-type=\"fig\"}A). The incorporation of a UMI sequence into the DNA is performed during the first round of PCR amplification. During this step, each template molecule is amplified with a primer that contains a UMI sequence at\n",
      "---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "torch.Size([1, 128])\n",
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Come up with a series of tasks: \n",
      "1. Are UMIs not specific to certain genes? Can the same UMI map to different genes?\n",
      "\n",
      "2. Are all UMIs equally efficient?\n",
      "\n",
      "3. Does each UMI need to be unique?\n",
      "\n",
      "4. Can more than one UMI be used per gene?\n",
      "\n",
      "To answer these questions, we designed a series of experiments to measure the efficiency and specificity of UMIs. We first evaluated the efficiency of UMIs as a function of the number of unique molecules per cell in the pool. We then evaluated\n",
      "---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "torch.Size([1, 128])\n",
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Come up with a series of tasks: \n",
      "1. Why do we need to barcode a read transcript too? Isn’t mapping it against the reference genome enough?\n",
      "2. What software does barcoding require?3. How is barcoding a computational problem?4. How do we get a good result with barcoding?The authors confirm that all data underlying the findings are fully available without restriction. All relevant data are within the paper and its Supporting Information file.\n",
      "\n",
      "Introduction {#s1}\n",
      "=======\n",
      "---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "torch.Size([1, 128])\n",
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Come up with a series of tasks: \n",
      "1. Can the same UMI map to different mRNA molecules of the same gene?\n",
      "\n",
      "2. Does a UMI sequence map equally well to a number of different genes?\n",
      "\n",
      "3. Does a UMI sequence map equally well to different genes?\n",
      "\n",
      "4. Do the transcripts of a gene that contain the same UMI sequence map to the same set of transcript reads?\n",
      "\n",
      "5. Is the UMI sequence at the 5' end of a read associated with a specific gene?\n",
      "\n",
      "These questions are answered in a straightforward manner by the\n",
      "---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "torch.Size([1, 128])\n",
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Come up with a series of tasks: \n",
      "1. Why do we need to barcode a read transcript too? Isn’t mapping it against the reference genome enough?\n",
      "2. Why do we need to map back the reads to the reference genome? What is the best reference genome?\n",
      "=======================================================\n",
      "---------\n",
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "torch.Size([1, 128])\n",
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Come up with a series of tasks: \n",
      "1. Imagine that we forgot to filter the VCFs to contain only fixed variants, and there are also SNPs with frequencies, of 15%, 30%, or 56.78%. Which allele do you think bcftools consensus would insert in the genome?\n",
      "\n",
      "2. Imagine that you can now generate a VCF file that contains only non-synonymous SNPs. Which allele do you think the consensus should be for this type of variant?\n",
      "\n",
      "3. Imagine you have a\n",
      "---------\n"
     ]
    }
   ],
   "source": [
    "# similarities = {}\n",
    "scorer = rouge_scorer.RougeScorer([\"rougeL\"], use_stemmer=False)\n",
    "machine_instructions = []\n",
    "# now let's generate new instructions!\n",
    "progress_bar = tqdm.tqdm(total=args[\"num_instructions_to_generate\"])\n",
    "if machine_instructions:\n",
    "    progress_bar.update(len(machine_instructions))\n",
    "\n",
    "with open(os.path.join(args[\"batch_dir\"], \"machine_generated_instructions.jsonl\"), \"a\") as fout:\n",
    "    #while len(machine_instructions) < args[\"num_instructions_to_generate\"]:\n",
    "    batch_inputs = []\n",
    "    for _ in range(args[\"request_batch_size\"]):\n",
    "        prompt_instructions = sample_machine_instructions(\n",
    "            machine_instructions, \n",
    "            similarities=None,\n",
    "            n=2)\n",
    "        # sample human instructions from the pool\n",
    "        prompt_instructions += random.sample(seed_instructions, args[\"num_prompt_instructions\"] - len(prompt_instructions))\n",
    "        random.shuffle(prompt_instructions)\n",
    "        prompt = encode_prompt(prompt_instructions, classification=args[\"use_clf_seed_tasks_only\"])\n",
    "        #print(\"Response:\", evaluate(prompt))\n",
    "        print(evaluate_pubmedgpt(prompt))\n",
    "        print(\"---------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f35b61-63ec-4ebf-9be8-58d8d3480869",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57557117-1871-4260-a64e-b98f385d1894",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52215da-191e-4fe9-bb0a-beeccfc14609",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''input_ids = tokenizer.encode(\n",
    "    \"Photosynthesis is \", return_tensors=\"pt\"\n",
    ").to(device)\n",
    "\n",
    "sample_output = model.generate(input_ids, do_sample=True, max_length=128, top_k=50)\n",
    "\n",
    "print(\"Output:\\n\" + 100 * \"-\")\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c22733-b78a-4910-a553-158b890e0540",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736b7721-382a-4f49-b70d-a84df448fbf4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb058f2-e2f0-42a0-b885-ae730f2e5632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing code for readme\n",
    "#for instruction in [\n",
    "#    \"Tell me about alpacas.\",\n",
    "#    \"Tell me about the president of Mexico in 2019.\",\n",
    "    #\"Tell me about the king of France in 2019.\",\n",
    "    #\"List all Canadian provinces in alphabetical order.\",\n",
    "    #\"Write a Python program that prints the first 10 Fibonacci numbers.\",\n",
    "    #\"Write a program that prints the numbers from 1 to 100. But for multiples of three print 'Fizz' instead of the number and for the multiples of five print 'Buzz'. For numbers which are multiples of both three and five print 'FizzBuzz'.\",  # noqa: E501\n",
    "    #\"Tell me five words that rhyme with 'shock'.\",\n",
    "    #\"Translate the sentence 'I have no mouth but I must scream' into Spanish.\",\n",
    "    #\"Count up from 1 to 500.\",\n",
    "#]:\n",
    "#    print(\"Instruction:\", instruction)\n",
    "#    print(\"Response:\", evaluate(instruction))\n",
    "#    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971b0f64-c8ea-4aa9-a8f1-351b42c54aec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca979ad-5305-4e46-9e29-05357efc99df",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''seed_tasks = [json.loads(l) for l in open(args[\"seed_tasks_path\"], \"r\")]\n",
    "if args[\"use_clf_seed_tasks_only\"]:\n",
    "    seed_tasks = [t for t in seed_tasks if t[\"is_classification\"]]\n",
    "seed_instructions = [t[\"instruction\"] for t in seed_tasks]\n",
    "print(f\"Loaded {len(seed_instructions)} human-written seed instructions\")\n",
    "\n",
    "os.makedirs(args[\"batch_dir\"], exist_ok=True)\n",
    "request_idx = 0\n",
    "# load the LM-generated instructions\n",
    "machine_instructions = []\n",
    "if os.path.exists(os.path.join(args[\"batch_dir\"], \"machine_generated_instructions.jsonl\")):\n",
    "    with open(os.path.join(args[\"batch_dir\"], \"machine_generated_instructions.jsonl\"), \"r\") as fin:\n",
    "        for line in fin:\n",
    "            instruction_info = json.loads(line)\n",
    "            machine_instructions.append(instruction_info[\"instruction\"])\n",
    "            request_idx = instruction_info[\"request_idx\"] + 1\n",
    "    print(f\"Loaded {len(machine_instructions)} machine-generated instructions\")\n",
    "\n",
    "# similarities = {}\n",
    "scorer = rouge_scorer.RougeScorer([\"rougeL\"], use_stemmer=False)\n",
    "\n",
    "# now let's generate new instructions!\n",
    "progress_bar = tqdm.tqdm(total=args[\"num_instructions_to_generate\"])\n",
    "if machine_instructions:\n",
    "    progress_bar.update(len(machine_instructions))\n",
    "\n",
    "with open(os.path.join(args[\"batch_dir\"], \"machine_generated_instructions.jsonl\"), \"a\") as fout:\n",
    "    #while len(machine_instructions) < args[\"num_instructions_to_generate\"]:\n",
    "    batch_inputs = []\n",
    "    for _ in range(args[\"request_batch_size\"]):\n",
    "        # sample machine instructions from the pool\n",
    "        prompt_instructions = sample_machine_instructions(\n",
    "            machine_instructions, \n",
    "            similarities=None,\n",
    "            n=2)\n",
    "        print(\"prompt_instructions\")\n",
    "        print(prompt_instructions)\n",
    "        print()\n",
    "        # sample human instructions from the pool\n",
    "        prompt_instructions += random.sample(seed_instructions, args[\"num_prompt_instructions\"] - len(prompt_instructions))\n",
    "        random.shuffle(prompt_instructions)\n",
    "        prompt = encode_prompt(prompt_instructions, classification=args[\"use_clf_seed_tasks_only\"])\n",
    "        print(\"prompt\")\n",
    "        print(prompt)\n",
    "        print()\n",
    "        batch_inputs.append(prompt)\n",
    "\n",
    "        sequences = pipeline(\n",
    "            #'I liked \"Breaking Bad\" and \"Band of Brothers\". Do you have any recommendations of other shows I might like?\\n',\n",
    "            batch_inputs,\n",
    "            do_sample=True,\n",
    "            top_k=10,\n",
    "            num_return_sequences=1,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            max_length=200,\n",
    "        )\n",
    "        print(\"Responses:\")\n",
    "        print(sequences)\n",
    "        break'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af161191-2509-4a65-b85f-efc53807dcd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67d7f6b-3cd4-4227-a8cf-d10f396ddc82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f811be0-2313-4183-ba1f-0e2e6c98286b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
