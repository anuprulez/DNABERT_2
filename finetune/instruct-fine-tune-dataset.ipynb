{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1748588-c875-4ca7-80e2-7dac121b5d78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/users/anup/miniconda3/envs/finetune-dnabert2/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "import tqdm\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from multiprocessing import Pool\n",
    "#from functools import partial\n",
    "from rouge_score import rouge_scorer\n",
    "#from gpt3_api import make_requests as make_gpt3_requests\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, GenerationConfig, LlamaForCausalLM, LlamaTokenizer\n",
    "\n",
    "import torch\n",
    "from peft import PeftModel\n",
    "\n",
    "\n",
    "random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e19e5cfd-ae87-4792-a9c0-21048ff4fbd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \n",
      "The class this function is called from is 'LlamaTokenizer'.\n",
      "Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 33/33 [00:15<00:00,  2.13it/s]\n"
     ]
    }
   ],
   "source": [
    "#from utils.callbacks import Iteratorize, Stream\n",
    "#from utils.prompter import Prompter\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"CPU\"\n",
    "\n",
    "load_8bit = True\n",
    "base_model = \"decapoda-research/llama-7b-hf\" #\"decapoda-research/llama-7b-hf\"\n",
    "lora_weights = \"tloen/alpaca-lora-7b\" #\"tloen/alpaca-lora-7b\"\n",
    "\n",
    "assert (\n",
    "    base_model\n",
    "), \"Please specify a --base_model, e.g. --base_model='huggyllama/llama-7b'\"\n",
    "\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(base_model)\n",
    "\n",
    "if device == \"cuda\":\n",
    "    model = LlamaForCausalLM.from_pretrained(\n",
    "        base_model,\n",
    "        load_in_8bit=load_8bit,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "    model = PeftModel.from_pretrained(\n",
    "        model,\n",
    "        lora_weights,\n",
    "        torch_dtype=torch.float16,\n",
    "    )\n",
    "elif device == \"mps\":\n",
    "    model = LlamaForCausalLM.from_pretrained(\n",
    "        base_model,\n",
    "        device_map={\"\": device},\n",
    "        torch_dtype=torch.float16,\n",
    "    )\n",
    "    model = PeftModel.from_pretrained(\n",
    "        model,\n",
    "        lora_weights,\n",
    "        device_map={\"\": device},\n",
    "        torch_dtype=torch.float16,\n",
    "    )\n",
    "else:\n",
    "    model = LlamaForCausalLM.from_pretrained(\n",
    "        base_model, device_map={\"\": device}, low_cpu_mem_usage=True\n",
    "    )\n",
    "    model = PeftModel.from_pretrained(\n",
    "        model,\n",
    "        lora_weights,\n",
    "        device_map={\"\": device},\n",
    "    )\n",
    "\n",
    "# unwind broken decapoda-research config\n",
    "model.config.pad_token_id = tokenizer.pad_token_id = 0  # unk\n",
    "model.config.bos_token_id = 1\n",
    "model.config.eos_token_id = 2\n",
    "\n",
    "if not load_8bit:\n",
    "    model.half()  # seems to fix bugs for some users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eecf3453-d74c-4d20-8519-4bbd8794bb71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39499417-d2b0-4b60-8970-408719d41df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A dedicated helper to manage templates and prompt building.\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import os.path as osp\n",
    "from typing import Union\n",
    "\n",
    "\n",
    "class Prompter(object):\n",
    "    __slots__ = (\"template\", \"_verbose\")\n",
    "\n",
    "    def __init__(self, template_name: str = \"\", verbose: bool = False):\n",
    "        self._verbose = verbose\n",
    "        if not template_name:\n",
    "            # Enforce the default here, so the constructor can be called with '' and will not break.\n",
    "            template_name = \"alpaca\"\n",
    "        file_name = osp.join(\"templates\", f\"{template_name}.json\")\n",
    "        if not osp.exists(file_name):\n",
    "            raise ValueError(f\"Can't read {file_name}\")\n",
    "        with open(file_name) as fp:\n",
    "            self.template = json.load(fp)\n",
    "        if self._verbose:\n",
    "            print(\n",
    "                f\"Using prompt template {template_name}: {self.template['description']}\"\n",
    "            )\n",
    "\n",
    "    def generate_prompt(\n",
    "        self,\n",
    "        instruction: str,\n",
    "        input: Union[None, str] = None,\n",
    "        label: Union[None, str] = None,\n",
    "    ) -> str:\n",
    "        # returns the full prompt from instruction and optional input\n",
    "        # if a label (=response, =output) is provided, it's also appended.\n",
    "        if input:\n",
    "            res = self.template[\"prompt_input\"].format(\n",
    "                instruction=instruction, input=input\n",
    "            )\n",
    "        else:\n",
    "            res = self.template[\"prompt_no_input\"].format(\n",
    "                instruction=instruction\n",
    "            )\n",
    "        if label:\n",
    "            res = f\"{res}{label}\"\n",
    "        if self._verbose:\n",
    "            print(res)\n",
    "        return res\n",
    "\n",
    "    def get_response(self, output: str) -> str:\n",
    "        return output #output.split(self.template[\"response_split\"])[1].strip()\n",
    "\n",
    "prompt_template = \"alpaca_1\"\n",
    "prompter = Prompter(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1fa68d71-7793-41fe-b87c-875a210cbd13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_dir': 'data/gpt3_generations/', 'seed_tasks_path': 'data/galaxy-qa.jsonl', 'num_instructions_to_generate': 1, 'num_prompt_instructions': 5, 'request_batch_size': 5, 'use_clf_seed_tasks_only': False}\n"
     ]
    }
   ],
   "source": [
    "args = {\"batch_dir\": \"data/gpt3_generations/\",\n",
    "       \"seed_tasks_path\": \"data/galaxy-qa.jsonl\",\n",
    "       \"num_instructions_to_generate\": 1,\n",
    "        \"num_prompt_instructions\": 5,\n",
    "        \"request_batch_size\": 5,\n",
    "        \"use_clf_seed_tasks_only\": False\n",
    "       }\n",
    "print(args)\n",
    "\n",
    "def encode_prompt(prompt_instructions, classification=False):\n",
    "    \"\"\"Encode multiple prompt instructions into a single string.\"\"\"\n",
    "    if classification:\n",
    "        prompt = \"Come up with a series of classification tasks. Try to specify the possible output labels when possible.\\n\"\n",
    "    else:\n",
    "        #prompt = \"Come up with a series of tasks:\\n\"\n",
    "        prompt = \"Come up with a new question using scientific keywords mentioned in existing tasks: \\n\"\n",
    "        #\"Come up with a series of new tasks using scientific keywords mentioned in existing tasks: \\n\" \n",
    "        #\"Come up with a series of new tasks: \\n\"\n",
    "    for idx, instruction in enumerate(prompt_instructions):\n",
    "        instruction = re.sub(r\"\\s+\", \" \", instruction).strip().rstrip(\":\")\n",
    "        prompt += f\"{idx+1}. {instruction}\\n\"\n",
    "    #prompt += f\"{len(prompt_instructions) + 1}.\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def sample_machine_instructions(machine_instructions, similarities, n):\n",
    "    \"\"\"Sample n machine instructions from a list of machine instructions.\"\"\"\n",
    "    return random.sample(machine_instructions, min(n, len(machine_instructions)))\n",
    "\n",
    "\n",
    "def find_word_in_string(w, s):\n",
    "    return re.compile(r'\\b({0})\\b'.format(w), flags=re.IGNORECASE).search(s)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f8a646a-8cba-4287-9dd3-08cd0d81f65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(\n",
    "    instruction,\n",
    "    input=None,\n",
    "    temperature=0.7, #0.1,\n",
    "    top_p=0.5, #0.75,\n",
    "    top_k=40,\n",
    "    num_beams=4,\n",
    "    max_new_tokens=128, #128,\n",
    "    max_tokens=1024,\n",
    "    stream_output=False\n",
    "):\n",
    "    prompt = instruction #prompter.generate_prompt(instruction, input)\n",
    "    print(prompt)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "    \n",
    "    generation_config = GenerationConfig(\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        top_k=top_k,\n",
    "        num_beams=num_beams,\n",
    "    )\n",
    "\n",
    "    generate_params = {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"generation_config\": generation_config,\n",
    "        \"return_dict_in_generate\": True,\n",
    "        \"output_scores\": True,\n",
    "        \"max_new_tokens\": max_new_tokens,\n",
    "    }\n",
    "\n",
    "    # Without streaming\n",
    "    with torch.no_grad():\n",
    "        generation_output = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            generation_config=generation_config,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "        )\n",
    "    s = generation_output.sequences[0]\n",
    "    output = tokenizer.decode(s)\n",
    "    return prompter.get_response(output)\n",
    "    #return dir(generation_output)\n",
    "    #print(generation_output)\n",
    "    #s = generation_output.sequences[0]\n",
    "    #output = tokenizer.decode(generation_output)\n",
    "    #print(s)\n",
    "    #yield output #prompter.get_response(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "513f5d5c-ac9c-4b84-bd47-41ac0663a63d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 8 human-written seed instructions\n"
     ]
    }
   ],
   "source": [
    "seed_tasks = [json.loads(l) for l in open(args[\"seed_tasks_path\"], \"r\")]\n",
    "if args[\"use_clf_seed_tasks_only\"]:\n",
    "    seed_tasks = [t for t in seed_tasks if t[\"is_classification\"]]\n",
    "seed_instructions = [t[\"instruction\"] for t in seed_tasks]\n",
    "print(f\"Loaded {len(seed_instructions)} human-written seed instructions\")\n",
    "\n",
    "os.makedirs(args[\"batch_dir\"], exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f1a5756c-8d26-41e9-820b-be00158598ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                                                                             | 0/1 [15:36<?, ?it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Come up with a new question using scientific keywords mentioned in existing tasks: \n",
      "1. Are UMIs not specific to certain genes? Can the same UMI map to different genes?\n",
      "2. Imagine that we forgot to filter the VCFs to contain only fixed variants, and there are also SNPs with frequencies, of 15%, 30%, or 56.78%. Which allele do you think bcftools consensus would insert in the genome?\n",
      "3. What do UMIs do?\n",
      "4. TB Variant Filter reads the VCF and output only SNPs that have, at least, 90% frequency. How can the software extract such information from the VCF datasets?\n",
      "5. Why do we need to barcode a read transcript too? Isn’t mapping it against the reference genome enough?\n",
      "\n",
      "Response: <unk>Come up with a new question using scientific keywords mentioned in existing tasks: \n",
      "1. Are UMIs not specific to certain genes? Can the same UMI map to different genes?\n",
      "2. Imagine that we forgot to filter the VCFs to contain only fixed variants, and there are also SNPs with frequencies, of 15%, 30%, or 56.78%. Which allele do you think bcftools consensus would insert in the genome?\n",
      "3. What do UMIs do?\n",
      "4. TB Variant Filter reads the VCF and output only SNPs that have, at least, 90% frequency. How can the software extract such information from the VCF datasets?\n",
      "5. Why do we need to barcode a read transcript too? Isn’t mapping it against the reference genome enough?\n",
      "6. What is the difference between a SNP and an INDEL?\n",
      "7. What is the difference between a SNP and an INDEL?\n",
      "8. What is the difference between a SNP and an INDEL?\n",
      "9. What is the difference between a SNP and an INDEL?\n",
      "10. What is the difference between a SNP and an INDEL?\n",
      "11. What is the difference between a SNP and an INDEL?\n",
      "12. What is the difference between a SNP and an INDEL?\n",
      "13. What is the\n",
      "---------\n",
      "Come up with a new question using scientific keywords mentioned in existing tasks: \n",
      "1. Why do we need to barcode a read transcript too? Isn’t mapping it against the reference genome enough?\n",
      "2. Imagine that we forgot to filter the VCFs to contain only fixed variants, and there are also SNPs with frequencies, of 15%, 30%, or 56.78%. Which allele do you think bcftools consensus would insert in the genome?\n",
      "3. Are UMIs not specific to certain genes? Can the same UMI map to different genes?\n",
      "4. Does the distribution of target identifications differ from the decoy distribution?\n",
      "5. TB Variant Filter reads the VCF and output only SNPs that have, at least, 90% frequency. How can the software extract such information from the VCF datasets?\n",
      "\n",
      "Response: <unk>Come up with a new question using scientific keywords mentioned in existing tasks: \n",
      "1. Why do we need to barcode a read transcript too? Isn’t mapping it against the reference genome enough?\n",
      "2. Imagine that we forgot to filter the VCFs to contain only fixed variants, and there are also SNPs with frequencies, of 15%, 30%, or 56.78%. Which allele do you think bcftools consensus would insert in the genome?\n",
      "3. Are UMIs not specific to certain genes? Can the same UMI map to different genes?\n",
      "4. Does the distribution of target identifications differ from the decoy distribution?\n",
      "5. TB Variant Filter reads the VCF and output only SNPs that have, at least, 90% frequency. How can the software extract such information from the VCF datasets?\n",
      "6. How does TB Variant Filter decide which allele to insert in the genome?\n",
      "7. How does TB Variant Filter decide which allele to insert in the genome?\n",
      "8. How does TB Variant Filter decide which allele to insert in the genome?\n",
      "9. How does TB Variant Filter decide which allele to insert in the genome?\n",
      "10. How does TB Variant Filter decide which allele to insert in the genome?\n",
      "11. How does TB Variant Filter decide which allele to insert in the genome?\n",
      "\n",
      "---------\n",
      "Come up with a new question using scientific keywords mentioned in existing tasks: \n",
      "1. Are UMIs not specific to certain genes? Can the same UMI map to different genes?\n",
      "2. TB Variant Filter reads the VCF and output only SNPs that have, at least, 90% frequency. How can the software extract such information from the VCF datasets?\n",
      "3. What do UMIs do?\n",
      "4. Can the same UMI map to different mRNA molecules of the same gene?\n",
      "5. Does the distribution of target identifications differ from the decoy distribution?\n",
      "\n",
      "Response: <unk>Come up with a new question using scientific keywords mentioned in existing tasks: \n",
      "1. Are UMIs not specific to certain genes? Can the same UMI map to different genes?\n",
      "2. TB Variant Filter reads the VCF and output only SNPs that have, at least, 90% frequency. How can the software extract such information from the VCF datasets?\n",
      "3. What do UMIs do?\n",
      "4. Can the same UMI map to different mRNA molecules of the same gene?\n",
      "5. Does the distribution of target identifications differ from the decoy distribution?\n",
      "6. How can the software extract such information from the VCF datasets?\n",
      "---------\n",
      "Come up with a new question using scientific keywords mentioned in existing tasks: \n",
      "1. Why do we need to barcode a read transcript too? Isn’t mapping it against the reference genome enough?\n",
      "2. TB Variant Filter reads the VCF and output only SNPs that have, at least, 90% frequency. How can the software extract such information from the VCF datasets?\n",
      "3. Does the distribution of target identifications differ from the decoy distribution?\n",
      "4. Why is it important to know which cell a read came from?\n",
      "5. Can the same UMI map to different mRNA molecules of the same gene?\n",
      "\n",
      "Response: <unk>Come up with a new question using scientific keywords mentioned in existing tasks: \n",
      "1. Why do we need to barcode a read transcript too? Isn’t mapping it against the reference genome enough?\n",
      "2. TB Variant Filter reads the VCF and output only SNPs that have, at least, 90% frequency. How can the software extract such information from the VCF datasets?\n",
      "3. Does the distribution of target identifications differ from the decoy distribution?\n",
      "4. Why is it important to know which cell a read came from?\n",
      "5. Can the same UMI map to different mRNA molecules of the same gene?\n",
      "6. Why do we need to barcode a read transcript too? Isn’t mapping it against the reference genome enough?\n",
      "7. How can the software extract such information from the VCF datasets?\n",
      "8. Does the distribution of target identifications differ from the decoy distribution?\n",
      "9. Why is it important to know which cell a read came from?\n",
      "10. Can the same UMI map to different mRNA molecules of the same gene?\n",
      "11. Why do we need to barcode a read transcript too? Isn’t mapping it against the reference genome enough?\n",
      "---------\n",
      "Come up with a new question using scientific keywords mentioned in existing tasks: \n",
      "1. Does the distribution of target identifications differ from the decoy distribution?\n",
      "2. Why do we need to barcode a read transcript too? Isn’t mapping it against the reference genome enough?\n",
      "3. Imagine that we forgot to filter the VCFs to contain only fixed variants, and there are also SNPs with frequencies, of 15%, 30%, or 56.78%. Which allele do you think bcftools consensus would insert in the genome?\n",
      "4. What do UMIs do?\n",
      "5. Why is it important to know which cell a read came from?\n",
      "\n",
      "Response: <unk>Come up with a new question using scientific keywords mentioned in existing tasks: \n",
      "1. Does the distribution of target identifications differ from the decoy distribution?\n",
      "2. Why do we need to barcode a read transcript too? Isn’t mapping it against the reference genome enough?\n",
      "3. Imagine that we forgot to filter the VCFs to contain only fixed variants, and there are also SNPs with frequencies, of 15%, 30%, or 56.78%. Which allele do you think bcftools consensus would insert in the genome?\n",
      "4. What do UMIs do?\n",
      "5. Why is it important to know which cell a read came from?\n",
      "6. What is the difference between a reference genome and a reference transcriptome?\n",
      "7. What is the difference between a reference genome and a reference transcriptome?\n",
      "8. What is the difference between a reference genome and a reference transcriptome?\n",
      "9. What is the difference between a reference genome and a reference transcriptome?\n",
      "10. What is the difference between a reference genome and a reference transcriptome?\n",
      "11. What is the difference between a reference genome and a reference transcriptome?\n",
      "12. What is the difference between a reference genome\n",
      "---------\n"
     ]
    }
   ],
   "source": [
    "# similarities = {}\n",
    "scorer = rouge_scorer.RougeScorer([\"rougeL\"], use_stemmer=False)\n",
    "machine_instructions = []\n",
    "# now let's generate new instructions!\n",
    "progress_bar = tqdm.tqdm(total=args[\"num_instructions_to_generate\"])\n",
    "if machine_instructions:\n",
    "    progress_bar.update(len(machine_instructions))\n",
    "\n",
    "with open(os.path.join(args[\"batch_dir\"], \"machine_generated_instructions.jsonl\"), \"a\") as fout:\n",
    "    #while len(machine_instructions) < args[\"num_instructions_to_generate\"]:\n",
    "    batch_inputs = []\n",
    "    for _ in range(args[\"request_batch_size\"]):\n",
    "        prompt_instructions = sample_machine_instructions(\n",
    "            machine_instructions, \n",
    "            similarities=None,\n",
    "            n=2)\n",
    "        # sample human instructions from the pool\n",
    "        prompt_instructions += random.sample(seed_instructions, args[\"num_prompt_instructions\"] - len(prompt_instructions))\n",
    "        random.shuffle(prompt_instructions)\n",
    "        prompt = encode_prompt(prompt_instructions, classification=args[\"use_clf_seed_tasks_only\"])\n",
    "        print(\"Response:\", evaluate(prompt))\n",
    "        print(\"---------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f35b61-63ec-4ebf-9be8-58d8d3480869",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57557117-1871-4260-a64e-b98f385d1894",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb058f2-e2f0-42a0-b885-ae730f2e5632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing code for readme\n",
    "#for instruction in [\n",
    "#    \"Tell me about alpacas.\",\n",
    "#    \"Tell me about the president of Mexico in 2019.\",\n",
    "    #\"Tell me about the king of France in 2019.\",\n",
    "    #\"List all Canadian provinces in alphabetical order.\",\n",
    "    #\"Write a Python program that prints the first 10 Fibonacci numbers.\",\n",
    "    #\"Write a program that prints the numbers from 1 to 100. But for multiples of three print 'Fizz' instead of the number and for the multiples of five print 'Buzz'. For numbers which are multiples of both three and five print 'FizzBuzz'.\",  # noqa: E501\n",
    "    #\"Tell me five words that rhyme with 'shock'.\",\n",
    "    #\"Translate the sentence 'I have no mouth but I must scream' into Spanish.\",\n",
    "    #\"Count up from 1 to 500.\",\n",
    "#]:\n",
    "#    print(\"Instruction:\", instruction)\n",
    "#    print(\"Response:\", evaluate(instruction))\n",
    "#    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971b0f64-c8ea-4aa9-a8f1-351b42c54aec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca979ad-5305-4e46-9e29-05357efc99df",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''seed_tasks = [json.loads(l) for l in open(args[\"seed_tasks_path\"], \"r\")]\n",
    "if args[\"use_clf_seed_tasks_only\"]:\n",
    "    seed_tasks = [t for t in seed_tasks if t[\"is_classification\"]]\n",
    "seed_instructions = [t[\"instruction\"] for t in seed_tasks]\n",
    "print(f\"Loaded {len(seed_instructions)} human-written seed instructions\")\n",
    "\n",
    "os.makedirs(args[\"batch_dir\"], exist_ok=True)\n",
    "request_idx = 0\n",
    "# load the LM-generated instructions\n",
    "machine_instructions = []\n",
    "if os.path.exists(os.path.join(args[\"batch_dir\"], \"machine_generated_instructions.jsonl\")):\n",
    "    with open(os.path.join(args[\"batch_dir\"], \"machine_generated_instructions.jsonl\"), \"r\") as fin:\n",
    "        for line in fin:\n",
    "            instruction_info = json.loads(line)\n",
    "            machine_instructions.append(instruction_info[\"instruction\"])\n",
    "            request_idx = instruction_info[\"request_idx\"] + 1\n",
    "    print(f\"Loaded {len(machine_instructions)} machine-generated instructions\")\n",
    "\n",
    "# similarities = {}\n",
    "scorer = rouge_scorer.RougeScorer([\"rougeL\"], use_stemmer=False)\n",
    "\n",
    "# now let's generate new instructions!\n",
    "progress_bar = tqdm.tqdm(total=args[\"num_instructions_to_generate\"])\n",
    "if machine_instructions:\n",
    "    progress_bar.update(len(machine_instructions))\n",
    "\n",
    "with open(os.path.join(args[\"batch_dir\"], \"machine_generated_instructions.jsonl\"), \"a\") as fout:\n",
    "    #while len(machine_instructions) < args[\"num_instructions_to_generate\"]:\n",
    "    batch_inputs = []\n",
    "    for _ in range(args[\"request_batch_size\"]):\n",
    "        # sample machine instructions from the pool\n",
    "        prompt_instructions = sample_machine_instructions(\n",
    "            machine_instructions, \n",
    "            similarities=None,\n",
    "            n=2)\n",
    "        print(\"prompt_instructions\")\n",
    "        print(prompt_instructions)\n",
    "        print()\n",
    "        # sample human instructions from the pool\n",
    "        prompt_instructions += random.sample(seed_instructions, args[\"num_prompt_instructions\"] - len(prompt_instructions))\n",
    "        random.shuffle(prompt_instructions)\n",
    "        prompt = encode_prompt(prompt_instructions, classification=args[\"use_clf_seed_tasks_only\"])\n",
    "        print(\"prompt\")\n",
    "        print(prompt)\n",
    "        print()\n",
    "        batch_inputs.append(prompt)\n",
    "\n",
    "        sequences = pipeline(\n",
    "            #'I liked \"Breaking Bad\" and \"Band of Brothers\". Do you have any recommendations of other shows I might like?\\n',\n",
    "            batch_inputs,\n",
    "            do_sample=True,\n",
    "            top_k=10,\n",
    "            num_return_sequences=1,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            max_length=200,\n",
    "        )\n",
    "        print(\"Responses:\")\n",
    "        print(sequences)\n",
    "        break'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af161191-2509-4a65-b85f-efc53807dcd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67d7f6b-3cd4-4227-a8cf-d10f396ddc82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f811be0-2313-4183-ba1f-0e2e6c98286b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
