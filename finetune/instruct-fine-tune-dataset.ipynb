{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1748588-c875-4ca7-80e2-7dac121b5d78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/users/anup/miniconda3/envs/finetune-dnabert2/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "import tqdm\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from multiprocessing import Pool\n",
    "#from functools import partial\n",
    "from rouge_score import rouge_scorer\n",
    "#from gpt3_api import make_requests as make_gpt3_requests\n",
    "from transformers import AutoTokenizer\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e05cdc1-b2f0-47b5-9bcd-28f7b9ada15f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba79890d-8838-4d42-9ffc-eb913e278f94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_dir': 'data/gpt3_generations/', 'seed_tasks_path': 'data/galaxy-qa.jsonl', 'num_instructions_to_generate': 1, 'num_prompt_instructions': 2, 'request_batch_size': 1, 'use_clf_seed_tasks_only': False}\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "794c669b-bab0-4d10-bae5-10f5eac378f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1495005-5910-4865-a348-6094dd2364c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e19e5cfd-ae87-4792-a9c0-21048ff4fbd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \n",
      "The class this function is called from is 'LlamaTokenizer'.\n",
      "Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 33/33 [00:27<00:00,  1.19it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "from peft import PeftModel\n",
    "from transformers import GenerationConfig, LlamaForCausalLM, LlamaTokenizer\n",
    "\n",
    "#from utils.callbacks import Iteratorize, Stream\n",
    "#from utils.prompter import Prompter\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"CPU\"\n",
    "\n",
    "'''def main(\n",
    "    load_8bit: bool = False,\n",
    "    base_model: str = \"\",\n",
    "    lora_weights: str = \"tloen/alpaca-lora-7b\",\n",
    "    prompt_template: str = \"\",  # The prompt template to use, will default to alpaca.\n",
    "):'''\n",
    "\n",
    "load_8bit = True\n",
    "base_model = \"decapoda-research/llama-7b-hf\"\n",
    "lora_weights = \"tloen/alpaca-lora-7b\"\n",
    "\n",
    "\n",
    "assert (\n",
    "    base_model\n",
    "), \"Please specify a --base_model, e.g. --base_model='huggyllama/llama-7b'\"\n",
    "\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(base_model)\n",
    "\n",
    "if device == \"cuda\":\n",
    "    model = LlamaForCausalLM.from_pretrained(\n",
    "        base_model,\n",
    "        load_in_8bit=load_8bit,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "    model = PeftModel.from_pretrained(\n",
    "        model,\n",
    "        lora_weights,\n",
    "        torch_dtype=torch.float16,\n",
    "    )\n",
    "elif device == \"mps\":\n",
    "    model = LlamaForCausalLM.from_pretrained(\n",
    "        base_model,\n",
    "        device_map={\"\": device},\n",
    "        torch_dtype=torch.float16,\n",
    "    )\n",
    "    model = PeftModel.from_pretrained(\n",
    "        model,\n",
    "        lora_weights,\n",
    "        device_map={\"\": device},\n",
    "        torch_dtype=torch.float16,\n",
    "    )\n",
    "else:\n",
    "    model = LlamaForCausalLM.from_pretrained(\n",
    "        base_model, device_map={\"\": device}, low_cpu_mem_usage=True\n",
    "    )\n",
    "    model = PeftModel.from_pretrained(\n",
    "        model,\n",
    "        lora_weights,\n",
    "        device_map={\"\": device},\n",
    "    )\n",
    "\n",
    "# unwind broken decapoda-research config\n",
    "model.config.pad_token_id = tokenizer.pad_token_id = 0  # unk\n",
    "model.config.bos_token_id = 1\n",
    "model.config.eos_token_id = 2\n",
    "\n",
    "if not load_8bit:\n",
    "    model.half()  # seems to fix bugs for some users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eecf3453-d74c-4d20-8519-4bbd8794bb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\"batch_dir\": \"data/gpt3_generations/\",\n",
    "       \"seed_tasks_path\": \"data/galaxy-qa.jsonl\",\n",
    "       \"num_instructions_to_generate\": 3,\n",
    "        \"num_prompt_instructions\": 2,\n",
    "        \"request_batch_size\": 1,\n",
    "        \"use_clf_seed_tasks_only\": False\n",
    "       }\n",
    "\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "39499417-d2b0-4b60-8970-408719d41df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A dedicated helper to manage templates and prompt building.\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import os.path as osp\n",
    "from typing import Union\n",
    "\n",
    "\n",
    "class Prompter(object):\n",
    "    __slots__ = (\"template\", \"_verbose\")\n",
    "\n",
    "    def __init__(self, template_name: str = \"\", verbose: bool = False):\n",
    "        self._verbose = verbose\n",
    "        if not template_name:\n",
    "            # Enforce the default here, so the constructor can be called with '' and will not break.\n",
    "            template_name = \"alpaca\"\n",
    "        file_name = osp.join(\"templates\", f\"{template_name}.json\")\n",
    "        if not osp.exists(file_name):\n",
    "            raise ValueError(f\"Can't read {file_name}\")\n",
    "        with open(file_name) as fp:\n",
    "            self.template = json.load(fp)\n",
    "        if self._verbose:\n",
    "            print(\n",
    "                f\"Using prompt template {template_name}: {self.template['description']}\"\n",
    "            )\n",
    "\n",
    "    def generate_prompt(\n",
    "        self,\n",
    "        instruction: str,\n",
    "        input: Union[None, str] = None,\n",
    "        label: Union[None, str] = None,\n",
    "    ) -> str:\n",
    "        # returns the full prompt from instruction and optional input\n",
    "        # if a label (=response, =output) is provided, it's also appended.\n",
    "        if input:\n",
    "            res = self.template[\"prompt_input\"].format(\n",
    "                instruction=instruction, input=input\n",
    "            )\n",
    "        else:\n",
    "            res = self.template[\"prompt_no_input\"].format(\n",
    "                instruction=instruction\n",
    "            )\n",
    "        if label:\n",
    "            res = f\"{res}{label}\"\n",
    "        if self._verbose:\n",
    "            print(res)\n",
    "        return res\n",
    "\n",
    "    def get_response(self, output: str) -> str:\n",
    "        return output #output.split(self.template[\"response_split\"])[1].strip()\n",
    "\n",
    "prompt_template = \"alpaca_1\"\n",
    "prompter = Prompter(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1fa68d71-7793-41fe-b87c-875a210cbd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_prompt(prompt_instructions, classification=False):\n",
    "    \"\"\"Encode multiple prompt instructions into a single string.\"\"\"\n",
    "    if classification:\n",
    "        prompt = \"Come up with a series of classification tasks. Try to specify the possible output labels when possible.\\n\"\n",
    "    else:\n",
    "        #prompt = \"Come up with a series of tasks:\\n\"\n",
    "        prompt = \"\"\n",
    "    for idx, instruction in enumerate(prompt_instructions):\n",
    "        instruction = re.sub(r\"\\s+\", \" \", instruction).strip().rstrip(\":\")\n",
    "        prompt += f\"{idx+1}. {instruction}\\n\"\n",
    "    prompt += f\"{len(prompt_instructions) + 1}.\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def sample_machine_instructions(machine_instructions, similarities, n):\n",
    "    \"\"\"Sample n machine instructions from a list of machine instructions.\"\"\"\n",
    "    return random.sample(machine_instructions, min(n, len(machine_instructions)))\n",
    "\n",
    "\n",
    "def find_word_in_string(w, s):\n",
    "    return re.compile(r'\\b({0})\\b'.format(w), flags=re.IGNORECASE).search(s)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1f8a646a-8cba-4287-9dd3-08cd0d81f65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(\n",
    "    instruction,\n",
    "    input=None,\n",
    "    temperature=0.1,\n",
    "    top_p=0.75,\n",
    "    top_k=40,\n",
    "    num_beams=4,\n",
    "    max_new_tokens=128,\n",
    "    stream_output=False\n",
    "):\n",
    "    prompt = prompter.generate_prompt(instruction, input)\n",
    "    print(prompt)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "    \n",
    "    generation_config = GenerationConfig(\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        top_k=top_k,\n",
    "        num_beams=num_beams,\n",
    "    )\n",
    "\n",
    "    generate_params = {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"generation_config\": generation_config,\n",
    "        \"return_dict_in_generate\": True,\n",
    "        \"output_scores\": True,\n",
    "        \"max_new_tokens\": max_new_tokens,\n",
    "    }\n",
    "\n",
    "   # print(prompt, input_ids)\n",
    "\n",
    "    # Without streaming\n",
    "    with torch.no_grad():\n",
    "        generation_output = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            generation_config=generation_config,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "        )\n",
    "    s = generation_output.sequences[0]\n",
    "    output = tokenizer.decode(s)\n",
    "    return prompter.get_response(output)\n",
    "    #return dir(generation_output)\n",
    "    #print(generation_output)\n",
    "    #s = generation_output.sequences[0]\n",
    "    #output = tokenizer.decode(generation_output)\n",
    "    #print(s)\n",
    "    #yield output #prompter.get_response(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "513f5d5c-ac9c-4b84-bd47-41ac0663a63d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 8 human-written seed instructions\n"
     ]
    }
   ],
   "source": [
    "seed_tasks = [json.loads(l) for l in open(args[\"seed_tasks_path\"], \"r\")]\n",
    "if args[\"use_clf_seed_tasks_only\"]:\n",
    "    seed_tasks = [t for t in seed_tasks if t[\"is_classification\"]]\n",
    "seed_instructions = [t[\"instruction\"] for t in seed_tasks]\n",
    "print(f\"Loaded {len(seed_instructions)} human-written seed instructions\")\n",
    "\n",
    "os.makedirs(args[\"batch_dir\"], exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f1a5756c-8d26-41e9-820b-be00158598ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                                                                                                                | 0/1 [02:17<?, ?it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extend the series of questions mentioned in instructions:\n",
      "\n",
      "### Instruction:\n",
      "1. Why do we need to barcode a read transcript too? Isn’t mapping it against the reference genome enough?\n",
      "2. Does the distribution of target identifications differ from the decoy distribution?\n",
      "3.\n",
      "\n",
      "### Response:\n",
      "\n",
      "Response: <unk>Extend the series of questions mentioned in instructions:\n",
      "\n",
      "### Instruction:\n",
      "1. Why do we need to barcode a read transcript too? Isn’t mapping it against the reference genome enough?\n",
      "2. Does the distribution of target identifications differ from the decoy distribution?\n",
      "3.\n",
      "\n",
      "### Response:\n",
      "### Instruction:\n",
      "1. Barcoding a read transcript is necessary because it provides a unique identifier for each read transcript. This identifier can be used to link the read transcript to other data such as gene expression data.\n",
      "2. No, the distribution of target identifications does not differ from the decoy distribution.\n",
      "3. No, the distribution of target identifications does not differ from the decoy distribution.\n"
     ]
    }
   ],
   "source": [
    "# similarities = {}\n",
    "scorer = rouge_scorer.RougeScorer([\"rougeL\"], use_stemmer=False)\n",
    "machine_instructions = []\n",
    "# now let's generate new instructions!\n",
    "progress_bar = tqdm.tqdm(total=args[\"num_instructions_to_generate\"])\n",
    "if machine_instructions:\n",
    "    progress_bar.update(len(machine_instructions))\n",
    "\n",
    "with open(os.path.join(args[\"batch_dir\"], \"machine_generated_instructions.jsonl\"), \"a\") as fout:\n",
    "    #while len(machine_instructions) < args[\"num_instructions_to_generate\"]:\n",
    "    batch_inputs = []\n",
    "    for _ in range(args[\"request_batch_size\"]):\n",
    "        # sample machine instructions from the pool\n",
    "        prompt_instructions = sample_machine_instructions(\n",
    "            machine_instructions, \n",
    "            similarities=None,\n",
    "            n=2)\n",
    "        #print(\"prompt_instructions\")\n",
    "        #print(prompt_instructions)\n",
    "        #print()\n",
    "        # sample human instructions from the pool\n",
    "        prompt_instructions += random.sample(seed_instructions, args[\"num_prompt_instructions\"] - len(prompt_instructions))\n",
    "        random.shuffle(prompt_instructions)\n",
    "        prompt = encode_prompt(prompt_instructions, classification=args[\"use_clf_seed_tasks_only\"])\n",
    "        #print(\"prompt\")\n",
    "        #print(prompt)\n",
    "        #print()\n",
    "        #batch_inputs.append(prompt)\n",
    "        #print(batch_inputs)\n",
    "        print(\"Response:\", evaluate(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f35b61-63ec-4ebf-9be8-58d8d3480869",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "57557117-1871-4260-a64e-b98f385d1894",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8eb058f2-e2f0-42a0-b885-ae730f2e5632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing code for readme\n",
    "#for instruction in [\n",
    "#    \"Tell me about alpacas.\",\n",
    "#    \"Tell me about the president of Mexico in 2019.\",\n",
    "    #\"Tell me about the king of France in 2019.\",\n",
    "    #\"List all Canadian provinces in alphabetical order.\",\n",
    "    #\"Write a Python program that prints the first 10 Fibonacci numbers.\",\n",
    "    #\"Write a program that prints the numbers from 1 to 100. But for multiples of three print 'Fizz' instead of the number and for the multiples of five print 'Buzz'. For numbers which are multiples of both three and five print 'FizzBuzz'.\",  # noqa: E501\n",
    "    #\"Tell me five words that rhyme with 'shock'.\",\n",
    "    #\"Translate the sentence 'I have no mouth but I must scream' into Spanish.\",\n",
    "    #\"Count up from 1 to 500.\",\n",
    "#]:\n",
    "#    print(\"Instruction:\", instruction)\n",
    "#    print(\"Response:\", evaluate(instruction))\n",
    "#    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971b0f64-c8ea-4aa9-a8f1-351b42c54aec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4ca979ad-5305-4e46-9e29-05357efc99df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/users/anup/miniconda3/envs/finetune-dnabert2/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      5\u001b[0m seed_tasks \u001b[38;5;241m=\u001b[39m [json\u001b[38;5;241m.\u001b[39mloads(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed_tasks_path\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_clf_seed_tasks_only\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m/scratch/users/anup/miniconda3/envs/finetune-dnabert2/lib/python3.9/site-packages/torch/__init__.py:1146\u001b[0m\n\u001b[1;32m   1143\u001b[0m py_int \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m\n\u001b[1;32m   1145\u001b[0m \u001b[38;5;66;03m# Shared memory manager needs to know the exact location of manager executable\u001b[39;00m\n\u001b[0;32m-> 1146\u001b[0m \u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initExtension\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmanager_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m manager_path\n\u001b[1;32m   1149\u001b[0m \u001b[38;5;66;03m# Appease the type checker: it can't deal with direct setting of globals().\u001b[39;00m\n\u001b[1;32m   1150\u001b[0m \u001b[38;5;66;03m# Note that we will see \"too many\" functions when reexporting this way; there\u001b[39;00m\n\u001b[1;32m   1151\u001b[0m \u001b[38;5;66;03m# is not a good way to fix this problem.  Perhaps, try to redesign VariableFunctions\u001b[39;00m\n\u001b[1;32m   1152\u001b[0m \u001b[38;5;66;03m# so that this import is good enough\u001b[39;00m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:203\u001b[0m, in \u001b[0;36m_lock_unlock_module\u001b[0;34m(name)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''seed_tasks = [json.loads(l) for l in open(args[\"seed_tasks_path\"], \"r\")]\n",
    "if args[\"use_clf_seed_tasks_only\"]:\n",
    "    seed_tasks = [t for t in seed_tasks if t[\"is_classification\"]]\n",
    "seed_instructions = [t[\"instruction\"] for t in seed_tasks]\n",
    "print(f\"Loaded {len(seed_instructions)} human-written seed instructions\")\n",
    "\n",
    "os.makedirs(args[\"batch_dir\"], exist_ok=True)\n",
    "request_idx = 0\n",
    "# load the LM-generated instructions\n",
    "machine_instructions = []\n",
    "if os.path.exists(os.path.join(args[\"batch_dir\"], \"machine_generated_instructions.jsonl\")):\n",
    "    with open(os.path.join(args[\"batch_dir\"], \"machine_generated_instructions.jsonl\"), \"r\") as fin:\n",
    "        for line in fin:\n",
    "            instruction_info = json.loads(line)\n",
    "            machine_instructions.append(instruction_info[\"instruction\"])\n",
    "            request_idx = instruction_info[\"request_idx\"] + 1\n",
    "    print(f\"Loaded {len(machine_instructions)} machine-generated instructions\")\n",
    "\n",
    "# similarities = {}\n",
    "scorer = rouge_scorer.RougeScorer([\"rougeL\"], use_stemmer=False)\n",
    "\n",
    "# now let's generate new instructions!\n",
    "progress_bar = tqdm.tqdm(total=args[\"num_instructions_to_generate\"])\n",
    "if machine_instructions:\n",
    "    progress_bar.update(len(machine_instructions))\n",
    "\n",
    "with open(os.path.join(args[\"batch_dir\"], \"machine_generated_instructions.jsonl\"), \"a\") as fout:\n",
    "    #while len(machine_instructions) < args[\"num_instructions_to_generate\"]:\n",
    "    batch_inputs = []\n",
    "    for _ in range(args[\"request_batch_size\"]):\n",
    "        # sample machine instructions from the pool\n",
    "        prompt_instructions = sample_machine_instructions(\n",
    "            machine_instructions, \n",
    "            similarities=None,\n",
    "            n=2)\n",
    "        print(\"prompt_instructions\")\n",
    "        print(prompt_instructions)\n",
    "        print()\n",
    "        # sample human instructions from the pool\n",
    "        prompt_instructions += random.sample(seed_instructions, args[\"num_prompt_instructions\"] - len(prompt_instructions))\n",
    "        random.shuffle(prompt_instructions)\n",
    "        prompt = encode_prompt(prompt_instructions, classification=args[\"use_clf_seed_tasks_only\"])\n",
    "        print(\"prompt\")\n",
    "        print(prompt)\n",
    "        print()\n",
    "        batch_inputs.append(prompt)\n",
    "\n",
    "        sequences = pipeline(\n",
    "            #'I liked \"Breaking Bad\" and \"Band of Brothers\". Do you have any recommendations of other shows I might like?\\n',\n",
    "            batch_inputs,\n",
    "            do_sample=True,\n",
    "            top_k=10,\n",
    "            num_return_sequences=1,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            max_length=200,\n",
    "        )\n",
    "        print(\"Responses:\")\n",
    "        print(sequences)\n",
    "        break'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af161191-2509-4a65-b85f-efc53807dcd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67d7f6b-3cd4-4227-a8cf-d10f396ddc82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f811be0-2313-4183-ba1f-0e2e6c98286b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
