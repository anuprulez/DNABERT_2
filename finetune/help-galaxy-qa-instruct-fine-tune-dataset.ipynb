{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1748588-c875-4ca7-80e2-7dac121b5d78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/users/anup/miniconda3/envs/finetune-dnabert2/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "import tqdm\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from multiprocessing import Pool\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, GenerationConfig, LlamaForCausalLM, LlamaTokenizer\n",
    "\n",
    "import torch\n",
    "from peft import PeftModel\n",
    "\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa33fab4-7974-407e-b29d-edca00295a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"stanford-crfm/BioMedLM\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"stanford-crfm/BioMedLM\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fa68d71-7793-41fe-b87c-875a210cbd13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_dir': 'data/gpt3_generations/', 'seed_tasks_path': 'data/help-galaxy-qa.jsonl', 'num_instructions_to_generate': 1, 'num_prompt_instructions': 1, 'request_batch_size': 20, 'use_clf_seed_tasks_only': False}\n"
     ]
    }
   ],
   "source": [
    "args = {\n",
    "    \"batch_dir\": \"data/gpt3_generations/\",\n",
    "    \"seed_tasks_path\": \"data/help-galaxy-qa.jsonl\",\n",
    "    \"num_instructions_to_generate\": 1,\n",
    "    \"num_prompt_instructions\": 1,\n",
    "    \"request_batch_size\": 20,\n",
    "    \"use_clf_seed_tasks_only\": False\n",
    "}\n",
    "\n",
    "print(args)\n",
    "\n",
    "def encode_prompt(prompt_instructions, classification=False):\n",
    "    \"\"\"Encode multiple prompt instructions into a single string.\"\"\"\n",
    "    if classification:\n",
    "        prompt = \"Come up with a series of classification tasks. Try to specify the possible output labels when possible.\\n\"\n",
    "    else:\n",
    "        #prompt = \"Come up with a series of tasks:\\n\"\n",
    "        prompt = \"Come up with a series of tasks: \\n\"\n",
    "        #\"Come up with a new question using scientific keywords mentioned in existing tasks: \\n\"\n",
    "        #\"Come up with a series of new tasks using scientific keywords mentioned in existing tasks: \\n\" \n",
    "        #\"Come up with a series of new tasks: \\n\"\n",
    "    for idx, instruction in enumerate(prompt_instructions):\n",
    "        instruction = re.sub(r\"\\s+\", \" \", instruction).strip().rstrip(\":\")\n",
    "        prompt += f\"{idx+1}. {instruction}\\n\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def sample_machine_instructions(machine_instructions, similarities, n):\n",
    "    \"\"\"Sample n machine instructions from a list of machine instructions.\"\"\"\n",
    "    return random.sample(machine_instructions, min(n, len(machine_instructions)))\n",
    "\n",
    "\n",
    "def find_word_in_string(w, s):\n",
    "    return re.compile(r'\\b({0})\\b'.format(w), flags=re.IGNORECASE).search(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f8a646a-8cba-4287-9dd3-08cd0d81f65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(\n",
    "    instruction,\n",
    "    input=None,\n",
    "    temperature=0.1, #0.7, #0.1,\n",
    "    top_p=0.75, #0.5, #0.75,\n",
    "    top_k=40,\n",
    "    num_beams=4,\n",
    "    max_new_tokens=128, #128,\n",
    "    #max_tokens=1024,\n",
    "    stream_output=False\n",
    "):\n",
    "    prompt = instruction\n",
    "    print(prompt)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "    \n",
    "    generation_config = GenerationConfig(\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        top_k=top_k,\n",
    "        num_beams=num_beams,\n",
    "    )\n",
    "\n",
    "    generate_params = {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"generation_config\": generation_config,\n",
    "        \"return_dict_in_generate\": True,\n",
    "        \"output_scores\": True,\n",
    "        \"max_new_tokens\": max_new_tokens,\n",
    "    }\n",
    "\n",
    "    # Without streaming\n",
    "    with torch.no_grad():\n",
    "        generation_output = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            generation_config=generation_config,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "        )\n",
    "    s = generation_output.sequences[0]\n",
    "    output = tokenizer.decode(s)\n",
    "    return output #prompter.get_response(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6d16d0c-02f4-4bb5-82c6-eabd925e8ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_pubmedgpt(instruction):\n",
    "    input_ids = tokenizer.encode(\n",
    "        instruction, return_tensors=\"pt\").to(device)\n",
    "    sample_output = model.generate(input_ids, do_sample=True, max_length=128, temperature=0.7, top_k=50)\n",
    "    print(\"Output:\\n\" + 100 * \"-\")\n",
    "    return tokenizer.decode(sample_output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "513f5d5c-ac9c-4b84-bd47-41ac0663a63d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5 human-written seed instructions\n"
     ]
    }
   ],
   "source": [
    "seed_tasks = [json.loads(l) for l in open(args[\"seed_tasks_path\"], \"r\")]\n",
    "if args[\"use_clf_seed_tasks_only\"]:\n",
    "    seed_tasks = [t for t in seed_tasks if t[\"is_classification\"]]\n",
    "seed_instructions = [t[\"instruction\"] for t in seed_tasks]\n",
    "print(f\"Loaded {len(seed_instructions)} human-written seed instructions\")\n",
    "\n",
    "os.makedirs(args[\"batch_dir\"], exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1a5756c-8d26-41e9-820b-be00158598ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Come up with a series of tasks: \n",
      "1. I setup a bed file (see below), but it does not work. I wonder how to correct it? Thanks. HPV16 1024 1043 HPV_1_LEFT 1 + GCDCARGGDCAYAAYAATGG HPV16 1183 1207 HPV_1_RIGHT 1 - GAAAAATAAACTGTAAATCATATTC\n",
      "\n",
      "2. I'm going to show you a file of the HPV16 reference genome (NC_001526.2) and a file of the HPV\n",
      "---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Come up with a series of tasks: \n",
      "1. I setup a bed file (see below), but it does not work. I wonder how to correct it? Thanks. HPV16 1024 1043 HPV_1_LEFT 1 + GCDCARGGDCAYAAYAATGG HPV16 1183 1207 HPV_1_RIGHT 1 - GAAAAATAAACTGTAAATCATATTC\n",
      "\n",
      "2. To analyze the next sequence you will need a list of features that contain the information you want to extract. This list can be found on the HPV webs\n",
      "---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Come up with a series of tasks: \n",
      "1. I have just done a set of nanopore sequencing with PCR based cDNAseq. Any suggestion for analysis? Can minimap2 do this job. I have the genomic assembly/interested contigs.\n",
      "\n",
      "2. I would like to see a discussion about the fact that the authors chose to use a reference genome assembly that is more distant in evolutionary history to the species you are looking at. Is that why they have to use a reference genome assembly that is more distant?\n",
      "\n",
      "3. If you are going to make a comparison\n",
      "---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Come up with a series of tasks: \n",
      "1. The Shovill tool does not work: Loading tool toolshed.g2.bx.psu.edu/repos/iuc/shovill/shovill/1.1.0+galaxy1 failed: Error: Internal Server Error (500). I wonder how it can go back to normal?\n",
      "\n",
      "2.  The Shovill tool does not work: Loading tool toolshed.g2.bx.psu.edu/repos/iuc\n",
      "---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Come up with a series of tasks: \n",
      "1. The Shovill tool does not work: Loading tool toolshed.g2.bx.psu.edu/repos/iuc/shovill/shovill/1.1.0+galaxy1 failed: Error: Internal Server Error (500). I wonder how it can go back to normal?\n",
      "\n",
      "2.  \\- What is the time required to run the Shovill tool?\n",
      "\n",
      "3.  \\- How would the Shovill tool perform on\n",
      "---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Come up with a series of tasks: \n",
      "1. The Shovill tool does not work: Loading tool toolshed.g2.bx.psu.edu/repos/iuc/shovill/shovill/1.1.0+galaxy1 failed: Error: Internal Server Error (500). I wonder how it can go back to normal?\n",
      "\n",
      "3.  This is not a proper benchmark: A simple dataset is used, which is not representative of a real application.\n",
      "\n",
      "4.  The results are not reported:\n",
      "---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\n",
      "Input length of input_ids is 179, but `max_length` is set to 128. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Come up with a series of tasks: \n",
      "1. I setup a bed file (see below), but it does not work. I wonder how to correct it? Thanks. HPV16 1024 1043 HPV_1_LEFT 1 + GCDCARGGDCAYAAYAATGG HPV16 1183 1207 HPV_1_RIGHT 1 - GAAAAATAAACTGTAAATCATATTC\n",
      "2. I am not sure what to do with the output file that I got from the command line. Thanks. HPV_1_LEFT\n",
      "---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Come up with a series of tasks: \n",
      "1. I am trying to run the second step of quantification (i.e. quantifier) following the mapper module of mirDeep2. In the mapping step everything is fine and I get the collapsed reads to be given to the quantifier module. Then, when trying to run the quantifier module, providing mature, precursor and star sequences, the run ends with an error which apparently lacks any suggestions on where the error is. Any clues on what’s going on here? I see a lot of unmapped sequences but this is somewhat expected from a biological point of view as the miRNA fraction is not usually the most abundant in the tissue I am investigating (zebrafish gonads)\n",
      "\n",
      "\n",
      "---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\n",
      "Input length of input_ids is 179, but `max_length` is set to 128. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Come up with a series of tasks: \n",
      "1. I setup a bed file (see below), but it does not work. I wonder how to correct it? Thanks. HPV16 1024 1043 HPV_1_LEFT 1 + GCDCARGGDCAYAAYAATGG HPV16 1183 1207 HPV_1_RIGHT 1 - GAAAAATAAACTGTAAATCATATTC\n",
      "\n",
      "2. My job is to check the quality of the data. The HPV16-genome has about 7000 bases.\n",
      "\n",
      "I have a problem with the\n",
      "---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\n",
      "Input length of input_ids is 187, but `max_length` is set to 128. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Come up with a series of tasks: \n",
      "1. I am trying to run the second step of quantification (i.e. quantifier) following the mapper module of mirDeep2. In the mapping step everything is fine and I get the collapsed reads to be given to the quantifier module. Then, when trying to run the quantifier module, providing mature, precursor and star sequences, the run ends with an error which apparently lacks any suggestions on where the error is. Any clues on what’s going on here? I see a lot of unmapped sequences but this is somewhat expected from a biological point of view as the miRNA fraction is not usually the most abundant in the tissue I am investigating (zebrafish gonads)\n",
      "\n",
      "\n",
      "---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Come up with a series of tasks: \n",
      "1. Hello, galaxy runs properly from my server, but when I try to access ir from local network I got error: “No supported WebSocket library detected. Please use ‘pip install uvicorn[standard]’, or install ‘websockets’ or ‘wsproto’ manually.” I think I have a problem with proxy_pass parameter: http://unix:/srv/galaxy/var/gunicorn.sock; Can you help me to properly configure this value? My galaxy is located in directory /home/biodata/galaxy. Should I change http://unix: to http://localhost:8080 or other value?\n",
      "\n",
      "\n",
      "---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Come up with a series of tasks: \n",
      "1. I setup a bed file (see below), but it does not work. I wonder how to correct it? Thanks. HPV16 1024 1043 HPV_1_LEFT 1 + GCDCARGGDCAYAAYAATGG HPV16 1183 1207 HPV_1_RIGHT 1 - GAAAAATAAACTGTAAATCATATTC\n",
      "\n",
      "2\\. Then, I opened a new folder, in which I added a new sample name, \"Unique\" (you can change that\n",
      "---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Come up with a series of tasks: \n",
      "1. I setup a bed file (see below), but it does not work. I wonder how to correct it? Thanks. HPV16 1024 1043 HPV_1_LEFT 1 + GCDCARGGDCAYAAYAATGG HPV16 1183 1207 HPV_1_RIGHT 1 - GAAAAATAAACTGTAAATCATATTC\n",
      "\n",
      "2\\. I put the program to run to generate a list of HPV16 variants, and it does not work. Thanks. HPV16 1024\n",
      "---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Come up with a series of tasks: \n",
      "1. I setup a bed file (see below), but it does not work. I wonder how to correct it? Thanks. HPV16 1024 1043 HPV_1_LEFT 1 + GCDCARGGDCAYAAYAATGG HPV16 1183 1207 HPV_1_RIGHT 1 - GAAAAATAAACTGTAAATCATATTC\n",
      "\n",
      "2. I will load the sequences of the HPV16 and HPV18 genomes into the Bowtie 2 version 2.3.4.1. I\n",
      "---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Come up with a series of tasks: \n",
      "1. The Shovill tool does not work: Loading tool toolshed.g2.bx.psu.edu/repos/iuc/shovill/shovill/1.1.0+galaxy1 failed: Error: Internal Server Error (500). I wonder how it can go back to normal?\n",
      "\n",
      "2.  Rephrase the description of the Shovill tool:\n",
      "\n",
      "3.  How do you use the tool?\n",
      "\n",
      "4.  What does the tool\n",
      "---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\n",
      "Input length of input_ids is 179, but `max_length` is set to 128. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Come up with a series of tasks: \n",
      "1. The Shovill tool does not work: Loading tool toolshed.g2.bx.psu.edu/repos/iuc/shovill/shovill/1.1.0+galaxy1 failed: Error: Internal Server Error (500). I wonder how it can go back to normal?\n",
      "\n",
      "2.  The Shovill tool needs to be developed further: Loading tool toolshed.g2.bx.psu.edu/repos/\n",
      "---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\n",
      "Input length of input_ids is 179, but `max_length` is set to 128. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Come up with a series of tasks: \n",
      "1. I am trying to run the second step of quantification (i.e. quantifier) following the mapper module of mirDeep2. In the mapping step everything is fine and I get the collapsed reads to be given to the quantifier module. Then, when trying to run the quantifier module, providing mature, precursor and star sequences, the run ends with an error which apparently lacks any suggestions on where the error is. Any clues on what’s going on here? I see a lot of unmapped sequences but this is somewhat expected from a biological point of view as the miRNA fraction is not usually the most abundant in the tissue I am investigating (zebrafish gonads)\n",
      "\n",
      "\n",
      "---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Come up with a series of tasks: \n",
      "1. I am trying to run the second step of quantification (i.e. quantifier) following the mapper module of mirDeep2. In the mapping step everything is fine and I get the collapsed reads to be given to the quantifier module. Then, when trying to run the quantifier module, providing mature, precursor and star sequences, the run ends with an error which apparently lacks any suggestions on where the error is. Any clues on what’s going on here? I see a lot of unmapped sequences but this is somewhat expected from a biological point of view as the miRNA fraction is not usually the most abundant in the tissue I am investigating (zebrafish gonads)\n",
      "\n",
      "\n",
      "---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\n",
      "Input length of input_ids is 179, but `max_length` is set to 128. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Come up with a series of tasks: \n",
      "1. I setup a bed file (see below), but it does not work. I wonder how to correct it? Thanks. HPV16 1024 1043 HPV_1_LEFT 1 + GCDCARGGDCAYAAYAATGG HPV16 1183 1207 HPV_1_RIGHT 1 - GAAAAATAAACTGTAAATCATATTC\n",
      "\n",
      "2. I also want to change the format of the sequence and the header of the file. Thanks. HPV16 1024 1043 HPV_\n",
      "---------\n",
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Come up with a series of tasks: \n",
      "1. I am trying to run the second step of quantification (i.e. quantifier) following the mapper module of mirDeep2. In the mapping step everything is fine and I get the collapsed reads to be given to the quantifier module. Then, when trying to run the quantifier module, providing mature, precursor and star sequences, the run ends with an error which apparently lacks any suggestions on where the error is. Any clues on what’s going on here? I see a lot of unmapped sequences but this is somewhat expected from a biological point of view as the miRNA fraction is not usually the most abundant in the tissue I am investigating (zebrafish gonads)\n",
      "\n",
      "\n",
      "---------\n"
     ]
    }
   ],
   "source": [
    "machine_instructions = []\n",
    "batch_inputs = []\n",
    "for _ in range(args[\"request_batch_size\"]):\n",
    "    prompt_instructions = []\n",
    "    prompt_instructions += random.sample(seed_instructions, args[\"num_prompt_instructions\"] - len(prompt_instructions))\n",
    "    random.shuffle(prompt_instructions)\n",
    "    prompt = encode_prompt(prompt_instructions, classification=args[\"use_clf_seed_tasks_only\"])\n",
    "    print(evaluate_pubmedgpt(prompt))\n",
    "    print(\"---------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c22733-b78a-4910-a553-158b890e0540",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736b7721-382a-4f49-b70d-a84df448fbf4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb058f2-e2f0-42a0-b885-ae730f2e5632",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971b0f64-c8ea-4aa9-a8f1-351b42c54aec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca979ad-5305-4e46-9e29-05357efc99df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af161191-2509-4a65-b85f-efc53807dcd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67d7f6b-3cd4-4227-a8cf-d10f396ddc82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f811be0-2313-4183-ba1f-0e2e6c98286b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
